# Code Walkthrough: How Everything Works Together

This document walks through the entire codebase, explaining how all the pieces fit together.

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                        ARCHITECTURE                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   main.py                                                        │
│      │                                                           │
│      ▼                                                           │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                    LLMClient                             │   │
│   │  ┌──────────────────────────────────────────────────┐   │   │
│   │  │  chat_completion()                                │   │   │
│   │  │     │                                             │   │   │
│   │  │     ├──► _stream_response() ─┐                    │   │   │
│   │  │     │                        │                    │   │   │
│   │  │     └──► _non_stream_response()                   │   │   │
│   │  │                              │                    │   │   │
│   │  └──────────────────────────────│────────────────────┘   │   │
│   └─────────────────────────────────│────────────────────────┘   │
│                                     │                            │
│                                     ▼                            │
│   ┌─────────────────────────────────────────────────────────┐   │
│   │                  StreamEvent                             │   │
│   │    type: EventType                                       │   │
│   │    text_delta: TextDelta | None                          │   │
│   │    error: str | None                                     │   │
│   │    finish_reason: str | None                             │   │
│   │    usage: TokenUsage | None                              │   │
│   └─────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## File-by-File Breakdown

### 1. `client/response.py` - Data Structures

This file defines all the data structures used to represent LLM responses.

```python
from __future__ import annotations  # Allows forward references in type hints
from dataclasses import dataclass
from enum import Enum
```

#### `TextDelta` - Holds streaming text content

```python
@dataclass
class TextDelta:
    content: str

    def __str__(self):
        return self.content
```

**Purpose:** When the LLM streams text, each chunk is wrapped in a `TextDelta`.

**Example:**

```python
delta = TextDelta(content="Hello")
print(delta)  # "Hello"
```

---

#### `EventType` - Defines possible event types

```python
@dataclass
class EventType(str, Enum):
    TEXT_DELTA = "text_delta"        # Partial text during streaming
    MESSAGE_COMPLETE = "message_complete"  # Final message
    ERROR = "error"                   # Error occurred
```

**Why it inherits from both `str` and `Enum`:**

- Makes it JSON-serializable
- Allows string comparison: `EventType.TEXT_DELTA == "text_delta"` is `True`

---

#### `TokenUsage` - Tracks API token usage

```python
@dataclass
class TokenUsage:
    prompt_tokens: int = 0      # Tokens in the input
    completion_tokens: int = 0  # Tokens generated by LLM
    total_tokens: int = 0       # prompt + completion
    cached_tokens: int = 0      # Tokens served from cache

    def __add__(self, other: TokenUsage) -> TokenUsage:
        # Allows: usage1 + usage2
        return TokenUsage(
            prompt_tokens=self.prompt_tokens + other.prompt_tokens,
            # ... etc
        )
```

**Purpose:** Track costs and monitor API usage.

---

#### `StreamEvent` - The main event container

```python
@dataclass
class StreamEvent:
    type: EventType                    # Required: what kind of event
    text_delta: TextDelta | None = None  # Optional: text content
    error: str | None = None             # Optional: error message
    finish_reason: str | None = None     # Optional: why generation stopped
    usage: TokenUsage | None = None      # Optional: token stats
```

**Purpose:** Unified structure for all events from the LLM.

---

### 2. `client/llm_client.py` - The LLM Client

This is the main class that handles communication with the LLM API.

#### Imports and Setup

```python
import asyncio
from typing import AsyncGenerator
from client.response import EventType, StreamEvent, TokenUsage, TextDelta
from typing import Any
from openai import AsyncOpenAI
import os
from openai import RateLimitError, APIConnectionError, APIError
```

---

#### The `LLMClient` Class

```python
class LLMClient:
    def __init__(self) -> None:
        self._client: AsyncOpenAI | None = None  # Lazy initialization
        self._max_retries: int = 3
```

**Lazy Initialization:** The client isn't created until first use. This saves resources if it's never used.

---

#### `get_client()` - Lazy Client Creation

```python
def get_client(self) -> AsyncOpenAI:
    if self._client is None:
        self._client = AsyncOpenAI(
            api_key=os.getenv("OPENROUTER_API_KEY"),
            base_url=os.getenv("OPENROUTER_BASE_URL"),
        )
    return self._client
```

**Pattern:** Singleton-like pattern. Only creates client once, reuses it afterward.

---

#### `close()` - Cleanup

```python
async def close(self) -> None:
    if self._client:
        await self._client.close()
        self._client = None
```

**Important:** Always close the client when done to free resources.

---

#### `chat_completion()` - Main Entry Point

```python
async def chat_completion(
    self, messages: list[dict[str, Any]], stream: bool = True
) -> AsyncGenerator[StreamEvent, None]:
```

**Parameters:**

- `messages`: The chat history (list of `{role, content}` dicts)
- `stream`: Whether to stream the response

**Returns:** An async generator that yields `StreamEvent` objects.

**The Flow:**

```python
client = self.get_client()
kwargs = {
    "model": os.getenv("MODEL_NAME"),
    "messages": messages,
    "stream": stream,
}

for attempt in range(self._max_retries + 1):
    try:
        if stream:
            async for event in self._stream_response(client, kwargs):
                yield event
        else:
            event = await self._non_stream_response(client, kwargs)
            yield event
```

---

#### Retry Logic with Exponential Backoff

```python
except RateLimitError as e:
    if attempt < self._max_retries:
        wait_time = 2**attempt  # 1s, 2s, 4s
        await asyncio.sleep(wait_time)
    else:
        yield StreamEvent(type=EventType.ERROR, error=f"Rate Limit: {e}")
        return
```

**Exponential Backoff:**

- Attempt 0: Wait 1 second (2^0)
- Attempt 1: Wait 2 seconds (2^1)
- Attempt 2: Wait 4 seconds (2^2)

---

#### `_stream_response()` - Handle Streaming

```python
async def _stream_response(
    self, client: AsyncOpenAI, kwargs: dict[str, Any]
) -> AsyncGenerator[StreamEvent, None]:
    # Make the API call
    response = await client.chat.completions.create(**kwargs)

    usage: TokenUsage | None = None
    finish_reason: str | None = None

    # Iterate over streaming chunks
    async for chunk in response:
        # Extract usage data if present
        if hasattr(chunk, "usage") and chunk.usage:
            usage = TokenUsage(...)

        if not chunk.choices:
            continue

        choice = chunk.choices[0]
        delta = choice.delta

        # Store finish reason
        if choice.finish_reason:
            finish_reason = choice.finish_reason

        # Yield text delta events
        if delta.content:
            yield StreamEvent(
                type=EventType.TEXT_DELTA,
                text_delta=TextDelta(content=delta.content),
            )

    # Yield completion event
    yield StreamEvent(
        type=EventType.MESSAGE_COMPLETE,
        finish_reason=finish_reason,
        usage=usage,
    )
```

**What happens:**

1. Make API call with `stream=True`
2. For each chunk, extract and yield text content
3. After all chunks, yield a completion event with stats

---

#### `_non_stream_response()` - Handle Non-Streaming

```python
async def _non_stream_response(
    self, client: AsyncOpenAI, kwargs: dict[str, Any]
) -> StreamEvent:
    response = await client.chat.completions.create(**kwargs)
    choice = response.choices[0]
    message = choice.message

    # Build text delta if content exists
    text_delta = None
    if message.content:
        text_delta = TextDelta(content=message.content)

    # Build usage stats if present
    usage = None
    if response.usage:
        usage = TokenUsage(...)

    return StreamEvent(
        type=EventType.MESSAGE_COMPLETE,
        text_delta=text_delta,
        finish_reason=choice.finish_reason,
        usage=usage,
    )
```

**Difference from streaming:** Returns a single complete response instead of multiple chunks.

---

### 3. `main.py` - Entry Point

```python
from client.llm_client import LLMClient
import asyncio
from dotenv import load_dotenv

# Load .env file into os.environ
load_dotenv()

async def main():
    client = LLMClient()

    messages = [{
        "role": "user",
        "content": "what's up?"
    }]

    # Consume the async generator
    async for event in client.chat_completion(messages=messages, stream=True):
        print(event)

    print("done")

# Run the async function
asyncio.run(main())
```

---

## Complete Request Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                      COMPLETE REQUEST FLOW                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. main.py calls chat_completion()                              │
│     │                                                            │
│     ▼                                                            │
│  2. LLMClient creates AsyncOpenAI client (if needed)            │
│     │                                                            │
│     ▼                                                            │
│  3. Calls OpenRouter API with stream=True                        │
│     │                                                            │
│     ▼                                                            │
│  4. API starts sending chunks                                    │
│     │                                                            │
│     ├──► Chunk 1: "Hello" ──► yield StreamEvent(TEXT_DELTA)     │
│     │                              │                             │
│     │                              ▼                             │
│     │                         main.py prints event               │
│     │                                                            │
│     ├──► Chunk 2: " World" ─► yield StreamEvent(TEXT_DELTA)     │
│     │                              │                             │
│     │                              ▼                             │
│     │                         main.py prints event               │
│     │                                                            │
│     └──► [done] ────────────► yield StreamEvent(MESSAGE_COMPLETE)│
│                                    │                             │
│                                    ▼                             │
│                               main.py prints "done"              │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Key Patterns Used

| Pattern             | Where Used          | Why                              |
| ------------------- | ------------------- | -------------------------------- |
| Lazy Initialization | `get_client()`      | Only create client when needed   |
| Async Generator     | `chat_completion()` | Stream data efficiently          |
| Exponential Backoff | Retry logic         | Graceful error recovery          |
| Dataclasses         | All data structures | Clean, type-safe data containers |
| Enum                | `EventType`         | Type-safe event identification   |
| Context Composition | `StreamEvent`       | Flexible event structure         |

---

## Extending the Code

### Adding a New Event Type

```python
# 1. Add to EventType enum
class EventType(str, Enum):
    TEXT_DELTA = "text_delta"
    MESSAGE_COMPLETE = "message_complete"
    ERROR = "error"
    TOOL_CALL = "tool_call"  # New!

# 2. Add field to StreamEvent if needed
@dataclass
class StreamEvent:
    type: EventType
    text_delta: TextDelta | None = None
    error: str | None = None
    finish_reason: str | None = None
    usage: TokenUsage | None = None
    tool_call: ToolCall | None = None  # New!

# 3. Handle in _stream_response
if delta.tool_calls:
    yield StreamEvent(
        type=EventType.TOOL_CALL,
        tool_call=ToolCall(...)
    )
```

---

### Adding System Prompts

```python
async def main():
    client = LLMClient()

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the weather like?"}
    ]

    async for event in client.chat_completion(messages=messages):
        if event.text_delta:
            print(event.text_delta.content, end="", flush=True)
```

---

## Summary

This codebase demonstrates:

1. **Clean Architecture**: Separation of data structures and business logic
2. **Async Best Practices**: Non-blocking I/O with proper generators
3. **Robust Error Handling**: Retries with exponential backoff
4. **Type Safety**: Extensive use of type hints
5. **Modern Python**: Dataclasses, Enums, async/await

The design allows for easy extension and modification while maintaining a clean, readable structure.
