from typing import AsyncGenerator
from client.response import EventType
from client.response import StreamEvent
from client.response import TokenUsage
from client.response import TextDelta
from typing import Any
from openai import AsyncOpenAI
import os

class LLMClient:
    def __init__(self) -> None:
        self._client : AsyncOpenAI | None = None
        pass
    
    # Creating the client if not created
    def get_client(self) -> AsyncOpenAI:
        if self._client is None:
            self._client = AsyncOpenAI(
                api_key=os.getenv("OPENROUTER_API_KEY"),
                base_url=os.getenv("OPENROUTER_BASE_URL")
            )
        return self._client
    
    # Closing the client if created 
    async def close(self) -> None:
        if self._client:
            await self._client.close()
            self._client = None

    
    async def chat_completion(self, messages: list[dict[str, Any]], stream: bool = True )-> AsyncGenerator[StreamEvent, None]:
        client = self.get_client()
        kwargs = {
            "model": os.getenv("MODEL_NAME"),
            "messages" : messages,
            "stream" : stream
        }
        if stream:
            await self._stream_response()
        else:
            event = await self._non_stream_response(client, kwargs)
            yield event
        return

    async def _stream_response(self):
        pass

    async def _non_stream_response(self, client: AsyncOpenAI, kwargs: dict[str, Any] ) -> StreamEvent:
        # it's configuration from the open router
        # **kwargs is used to pass the arguments to the function which converts dict into function parameters. 
        # **Kwargs is dictionary of named arguments.
        response = await client.chat.completions.create(**kwargs)
        choice = response.choices[0]
        message = choice.message
        text_delta = None
        if message.content:
            text_delta = TextDelta(content=message.content)

        usage = None
        if response.usage:
            usage = TokenUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens,
                cached_tokens=response.usage.prompt_tokens_details.cached_tokens
            )

        return StreamEvent(
            type= EventType.MESSAGE_COMPLETE,
            text_delta=text_delta,
            finish_reason=choice.finish_reason,
            usage=usage
        )

    
